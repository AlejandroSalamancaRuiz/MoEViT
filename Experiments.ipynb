{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import split_dataset_casia_wf, create_data_loader, split_dataset_digi_face\n",
    "from MoEViT import MoEViT, MoEViTConfig\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_shards(path_to_data, batch_size, img_dim, num_shards, output_folder):\n",
    "\n",
    "    data_transforms = transforms.Compose([transforms.Resize([img_dim, img_dim]),\n",
    "                                            transforms.ToTensor()])\n",
    "\n",
    "    # Load the dataset\n",
    "    data = ImageFolder(root=path_to_data, transform=data_transforms)\n",
    "    \n",
    "    # Create a data loader\n",
    "    data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Calculate total number of samples and the size of each shard\n",
    "    num_total_samples = len(data)\n",
    "    shard_size = num_total_samples // num_shards\n",
    "    remaining = num_total_samples % shard_size\n",
    "        \n",
    "    # Create tensors to hold the current shard data\n",
    "    tensor_data = torch.zeros((shard_size, 3, img_dim, img_dim))\n",
    "    tensor_labels = torch.zeros(shard_size, dtype=torch.long)\n",
    "    \n",
    "    current_shard_idx = 0\n",
    "    current_shard_count = 0\n",
    "\n",
    "    for img, lab in data_loader:\n",
    "\n",
    "        batch_size_current = img.size(0)\n",
    "        \n",
    "        if current_shard_count + batch_size_current <= shard_size:\n",
    "            # If the current batch fits into the current shard\n",
    "            tensor_data[current_shard_count:current_shard_count + batch_size_current] = img\n",
    "            tensor_labels[current_shard_count:current_shard_count + batch_size_current] = lab\n",
    "            current_shard_count += batch_size_current\n",
    "        else:\n",
    "\n",
    "            # Split the current batch across the current and the next shard\n",
    "            remaining_space = shard_size - current_shard_count\n",
    "            # Fill the current shard\n",
    "            if remaining_space > 0:\n",
    "                tensor_data[current_shard_count:current_shard_count + remaining_space] = img[:remaining_space]\n",
    "                tensor_labels[current_shard_count:current_shard_count + remaining_space] = lab[:remaining_space]\n",
    "            \n",
    "            save_shard(tensor_data.clone(), tensor_labels.clone(), output_folder, current_shard_idx)\n",
    "            \n",
    "            # Start a new shard with the remaining data\n",
    "            if current_shard_idx == num_shards - 1:\n",
    "                tensor_data = torch.zeros((shard_size + remaining, 3, img_dim, img_dim))\n",
    "                tensor_labels = torch.zeros(shard_size + remaining, dtype=torch.long)\n",
    "\n",
    "            tensor_data = torch.zeros((shard_size, 3, img_dim, img_dim))\n",
    "            tensor_labels = torch.zeros(shard_size, dtype=torch.long)\n",
    "            \n",
    "            current_shard_idx += 1\n",
    "            current_shard_count = batch_size_current - remaining_space\n",
    "            \n",
    "            tensor_data[:current_shard_count] = img[remaining_space:]\n",
    "            tensor_labels[:current_shard_count] = lab[remaining_space:]\n",
    "\n",
    "\n",
    "        # Save the last shard if it has remaining data\n",
    "    if current_shard_count > 0:\n",
    "        save_shard(tensor_data[:current_shard_count].clone(), tensor_labels[:current_shard_count].clone(), output_folder, current_shard_idx)\n",
    "\n",
    "    \n",
    "    return True\n",
    "\n",
    "def save_shard(images, labels, output_folder, shard_idx):    \n",
    "    # Save both images and labels in a single file\n",
    "    dataset = TensorDataset(images, labels)\n",
    "    shard_file = os.path.join(output_folder, f'shard_{shard_idx}.pt')\n",
    "    torch.save(dataset, shard_file)\n",
    "\n",
    "\n",
    "def load_shard(filename):\n",
    "    dataset = torch.load(filename)\n",
    "    images, labels = dataset.tensors\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, B, data_root, process_rank, num_processes):\n",
    "        self.B = B\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [os.path.join(data_root, s) for s in shards if s.endswith('.pt')]\n",
    "        self.shards = shards\n",
    "\n",
    "        self.current_shard = 0\n",
    "        self.imgs, self.labels = load_shard(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.process_rank\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "\n",
    "        x = self.imgs[self.current_position : self.current_position + B]\n",
    "        y = self.labels[self.current_position : self.current_position + B]\n",
    "\n",
    "        self.current_position += B * self.num_processes\n",
    "\n",
    "        if self.current_position + (B * self.num_processes) > len(self.labels):\n",
    "\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.imgs, self.labels = load_shard(self.shards[self.current_shard])\n",
    "            self.current_position = self.B * self.process_rank\n",
    "\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_dataset_casi_wf(base_folder='casia-web-face-dataset', validation_percentage=0.15, num_identification_imgs=500)\n",
    "#split_dataset_digi_face(base_folder='Digi-Face', validation_percentage=0.1, num_identification_imgs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "def donwload_data_oci(namespace, bucket, file, resource_principal=True):\n",
    "    \n",
    "    if resource_principal:\n",
    "    \n",
    "        # Initialize a Resource Principals provider\n",
    "        auth_provider = oci.auth.signers.get_resource_principals_signer()\n",
    "        # Create an Object Storage client\n",
    "        object_storage_client = oci.object_storage.ObjectStorageClient({}, signer=auth_provider)\n",
    "\n",
    "    # Get the object\n",
    "    response = object_storage_client.get_object(namespace, bucket, file)\n",
    "    \n",
    "    path_compressed = 'compressed_data'\n",
    "    path_uncompressed = 'data'\n",
    "    \n",
    "    # Save the object data to a file\n",
    "    with open('compressed_data', 'wb') as file:\n",
    "        for chunk in response.data.raw.stream(1024 * 1024, decode_content=False):\n",
    "            file.write(chunk)\n",
    "            \n",
    "    try:\n",
    "        with zipfile.ZipFile(path_compressed, 'r') as zip_ref:\n",
    "            zip_ref.extractall(path_uncompressed)\n",
    "            print(f\"Extracted all files to {path_uncompressed}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {path_compressed} was not found.\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Error: The file {path_compressed} is not a zip file or it is corrupted.\")\n",
    "            \n",
    "\n",
    "donwload_data_oci('lr1qux0xfjaq', 'bucket-casia-webface', 'casia-webface.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Counts the number of files in a directory and its subdirectories.\n",
    "    \n",
    "    Parameters:\n",
    "    directory_path (str): Path to the directory.\n",
    "    \n",
    "    Returns:\n",
    "    int: Number of files in the directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    file_count = 0\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        file_count += len(files)\n",
    "    return file_count\n",
    "\n",
    "# Example usage\n",
    "directory_path = 'data_unzip'\n",
    "number_of_files = count_files_in_directory(directory_path)\n",
    "print(f\"Number of files in '{directory_path}' and its subdirectories: {number_of_files}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
